{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW, Document-term matrix, TF-IDF. Based on tutorials: https://radimrehurek.com/gensim/tut1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test corpora based on: http://topicmodels.west.uni-koblenz.de/ckling/tmt/part1.pdf\n",
    "'''documents = [\"probabilistic topic model\",\n",
    "    \"probabilistic topic model\",\n",
    "    \"probabilistic topic model\",\n",
    "    \"probabilistic topic model\",\n",
    "    \"probabilistic topic model\",\n",
    "    \"probabilistic topic model\",\n",
    "    \"probabilistic topic model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\"\n",
    "]'''\n",
    "'''documents = [\"probabilistic topic model\",\n",
    "    \"probabilistic topic model\",\n",
    "    \"probabilistic topic model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\",\n",
    "    \"famous fashion model\"\n",
    "]'''\n",
    "documents = [\"modem the steering linux. modem, linux the modem. steering the modem. linux!\",\n",
    "    \"linux; the linux. the linux modem linux. the modem, clutch the modem. gear.\",\n",
    "    \"gear! clutch the steering, steering, linux. the steering clutch gear. clutch the gear; the clutch.\",\n",
    "    \"the the the. clutch clutch clutch! steering gear; steering gear gear; steering gear!!!!\"]\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['modem', 'the', 'steering', 'linux', 'modem', 'linux', 'the', 'modem', 'steering', 'the', 'modem', 'linux'], ['linux', 'the', 'linux', 'the', 'linux', 'modem', 'linux', 'the', 'modem', 'clutch', 'the', 'modem', 'gear'], ['gear', 'clutch', 'the', 'steering', 'steering', 'linux', 'the', 'steering', 'clutch', 'gear', 'clutch', 'the', 'gear', 'the', 'clutch'], ['the', 'the', 'the', 'clutch', 'clutch', 'clutch', 'steering', 'gear', 'steering', 'gear', 'gear', 'steering', 'gear']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "texts = [[word for word in word_tokenize(document) if word not in string.punctuation] for document in documents]\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narho_000\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'modem': 0, 'the': 1, 'steering': 2, 'linux': 3, 'clutch': 4, 'gear': 5}\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 4), (1, 3), (2, 2), (3, 3)],\n",
       " [(0, 3), (1, 4), (3, 4), (4, 1), (5, 1)],\n",
       " [(1, 4), (2, 3), (3, 1), (4, 4), (5, 3)],\n",
       " [(1, 3), (2, 3), (4, 3), (5, 4)]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = [dictionary.doc2bow(text) for text in texts]\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2dense\n",
    "bow_dense = corpus2dense(bow, len(dictionary.token2id), len(bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           D1   D2   D3   D4\n",
      "modem     4.0  3.0  0.0  0.0\n",
      "the       3.0  4.0  4.0  3.0\n",
      "steering  2.0  0.0  3.0  3.0\n",
      "linux     3.0  4.0  1.0  0.0\n",
      "clutch    0.0  1.0  4.0  3.0\n",
      "gear      0.0  1.0  3.0  4.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [f'D{i+1}' for i, doc in enumerate(bow)]\n",
    "bow_df = pd.DataFrame(bow_dense, columns = columns, index = dictionary.token2id)\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.936603022962913), (2, 0.19436268823376643), (3, 0.29154403235064963)],\n",
       " [(0, 0.8624176140851579),\n",
       "  (3, 0.47724753317857443),\n",
       "  (4, 0.11931188329464361),\n",
       "  (5, 0.11931188329464361)],\n",
       " [(2, 0.50709255283711),\n",
       "  (3, 0.1690308509457033),\n",
       "  (4, 0.6761234037828132),\n",
       "  (5, 0.50709255283711)],\n",
       " [(2, 0.5144957554275266), (4, 0.5144957554275266), (5, 0.6859943405700354)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "model = models.TfidfModel(bow)\n",
    "tfidf = [model[doc] for doc in bow]\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_dense = corpus2dense(tfidf, len(dictionary.token2id), len(bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                D1        D2        D3        D4\n",
      "modem     0.936603  0.862418  0.000000  0.000000\n",
      "the       0.000000  0.000000  0.000000  0.000000\n",
      "steering  0.194363  0.000000  0.507093  0.514496\n",
      "linux     0.291544  0.477248  0.169031  0.000000\n",
      "clutch    0.000000  0.119312  0.676123  0.514496\n",
      "gear      0.000000  0.119312  0.507093  0.685994\n"
     ]
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_dense, columns = columns, index = dictionary.token2id)\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD, LSA (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On bow matrix, with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.624*\"the\" + 0.384*\"clutch\" + 0.376*\"gear\" + 0.364*\"steering\" + 0.335*\"linux\" + 0.277*\"modem\"'),\n",
       " (1,\n",
       "  '0.583*\"modem\" + 0.514*\"linux\" + -0.412*\"gear\" + -0.397*\"clutch\" + -0.242*\"steering\" + 0.099*\"the\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi = models.LsiModel(bow, id2word=dictionary, num_topics=2) # initialize an LSI transformation\n",
    "lsi.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** Interpretation of the _topic loadings_ (the coefficients associated with each probable word in a topic) is a major difficulty with LSI/LSA - [some](https://www.quora.com/What-is-the-interpretation-of-negative-scores-in-the-basis-vectors-obtained-with-LSI/answer/Luis-Argerich?srid=3A71), like this gensim LSI model, assign words to topics using highest absolute value/magnitude. [Others](https://www.slideshare.net/vitomirkovanovic/topic-modeling-for-learning-analytics-researchers-lak15-tutorial/65) assign with the highest signed value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** Notice the effect of not cleaning up _stop words_ when running LSA on the term frequency matrix. 'the' has been determined to be the most important word in the most important topic (in both magnitude and signed interpretations of _topic loadings_), which rings false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Word->Loading bar charts for each topic (like http://topicmodels.west.uni-koblenz.de/ckling/tmt/part1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.27744519  0.5826725 ]\n",
      " [ 0.62416737  0.09912868]\n",
      " [ 0.36351351 -0.24249522]\n",
      " [ 0.33508594  0.51399463]\n",
      " [ 0.38393822 -0.39735845]\n",
      " [ 0.37630578 -0.4120414 ]]\n"
     ]
    }
   ],
   "source": [
    "print(lsi.projection.u) # left singular vectors (U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 11.25526786   6.53892079]\n"
     ]
    }
   ],
   "source": [
    "print(lsi.projection.s) # singular values (Ʃ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41887654  0.48240457  0.58523405  0.4993289 ]\n",
      " [ 0.56355929  0.51860406 -0.40412391 -0.50013462]]\n"
     ]
    }
   ],
   "source": [
    "corpus_lsi = lsi[bow] # create a wrapper over the original corpus: bow->fold-in-lsi\n",
    "\n",
    "# https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ#q3-how-do-you-calculate-the-matrix-v-in-lsi-space\n",
    "print((corpus2dense(corpus_lsi, len(lsi.projection.s)).T / lsi.projection.s).T) # right singular vectors (V^T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4.7145677241169315), (1, 3.6850694969601752)]\n",
      "[(0, 5.4295928192507432), (1, 3.3911108898747324)]\n",
      "[(0, 6.5869661866505869), (1, -2.6425342939042018)]\n",
      "[(0, 5.620080433766681), (1, -3.2703405625661404)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_lsi: # doc inference: bow->lsi transformation is actually executed here, on the fly\n",
    "     print(doc)\n",
    "        \n",
    "# TODO: why are these numbers so big (and not < 1)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On bow matrix, with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "from gensim.matutils import corpus2csc\n",
    "\n",
    "# requires sparse matrix form of bow\n",
    "bow_sparse = corpus2csc(bow)\n",
    "U, Sigma, VT = randomized_svd(bow_sparse, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.27744519  0.5826725 ]\n",
      " [ 0.62416737  0.09912868]\n",
      " [ 0.36351351 -0.24249522]\n",
      " [ 0.33508594  0.51399463]\n",
      " [ 0.38393822 -0.39735845]\n",
      " [ 0.37630578 -0.4120414 ]]\n"
     ]
    }
   ],
   "source": [
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 11.25526786   6.53892079]\n"
     ]
    }
   ],
   "source": [
    "print(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41887655  0.48240458  0.58523407  0.49932889]\n",
      " [ 0.56355928  0.51860406 -0.40412392 -0.5001346 ]]\n"
     ]
    }
   ],
   "source": [
    "print(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [TruncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) below to 1) perform dimensionality reduction on the doc-term matrix, 2) get extra characteristics like explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.12271994  3.81004931]\n",
      " [ 7.02517092  0.64819461]\n",
      " [ 4.09144197 -1.58565701]\n",
      " [ 3.77148204  3.36097017]\n",
      " [ 4.32132753 -2.59829542]\n",
      " [ 4.23542236 -2.69430611]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2, n_iter=1)\n",
    "svd.fit(bow_sparse)\n",
    "print(svd.transform(bow_sparse)) # dimensionality reduction on original bow matrix: word inference (TODO: right?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41887655  0.48240458  0.58523407  0.49932889]\n",
      " [ 0.56355928  0.51860406 -0.40412392 -0.5001346 ]]\n"
     ]
    }
   ],
   "source": [
    "'''SVD suffers from a problem called \"sign indeterminancy\", which means the\n",
    "sign of the ``components_`` and the output from transform depend on the\n",
    "algorithm and random state. To work around this, fit instances of this\n",
    "class to data once, then keep the instance around to do transformations.'''\n",
    "print(svd.components_) # V^T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: write about signification of explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 11.25526786   6.53892079]\n"
     ]
    }
   ],
   "source": [
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.50696752  7.10165316]\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15816569  0.74536301]\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On tf-idf matrix, with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.593*\"modem\" + 0.441*\"clutch\" + 0.438*\"gear\" + 0.405*\"steering\" + 0.314*\"linux\" + 0.000*\"the\"'),\n",
       " (1,\n",
       "  '-0.707*\"modem\" + 0.418*\"gear\" + 0.412*\"clutch\" + 0.316*\"steering\" + -0.236*\"linux\" + -0.000*\"the\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi = models.LsiModel(tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation\n",
    "lsi.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting results. On the bright side, tf-idf LSA correctly gives the stopword 'the' a loading of ~0, which bow LSA struggled with.\n",
    "\n",
    "However, the topics are not very interpretable: all words in topic 0 have very similar topic loadings, topic 1 has a clear separation between the word 'modem' and the rest of the field (from the magnitude perspective), and you can really only see the separation between the 'cars' and 'IT' words by using sign in topic 1.\n",
    "\n",
    "Comparing loadings per word across topics, the higher magnitudes mostly come from the same topic for this example (topic 0 - the only max that comes from topic 1 is for _modem_). **TODO:** is this cross-topic comparison valid, or should I stick with the separate word->loading bar charts per topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.92808679e-01  -7.06679855e-01]\n",
      " [  4.09109497e-17  -4.72574037e-17]\n",
      " [  4.04574539e-01   3.15748047e-01]\n",
      " [  3.13511892e-01  -2.36236614e-01]\n",
      " [  4.40860983e-01   4.12293569e-01]\n",
      " [  4.38462313e-01   4.18465087e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(lsi.projection.u) # left singular vectors (U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.50330866  1.28357349]\n"
     ]
    }
   ],
   "source": [
    "print(lsi.projection.s) # singular values (Ʃ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48244458  0.50939982  0.5179025   0.48942423]\n",
      " [-0.52149894 -0.48542378  0.47612706  0.51546638]]\n"
     ]
    }
   ],
   "source": [
    "corpus_lsi = lsi[tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "\n",
    "# https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ#q3-how-do-you-calculate-the-matrix-v-in-lsi-space\n",
    "print((corpus2dense(corpus_lsi, len(lsi.projection.s)).T / lsi.projection.s).T) # right singular vectors (V^T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.72526311682447664), (1, -0.66938222377208589)]\n",
      "[(0, 0.76578514191849489), (1, -0.62307711556749001)]\n",
      "[(0, 0.77856731972283111), (1, 0.61114406793638831)]\n",
      "[(0, 0.73575565287651956), (1, 0.66163900269050036)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_lsi: # doc inference: both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "     print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On tf-idf matrix, with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "# randomized_svd requires sparse matrix form of tfidf\n",
    "tfidf_sparse = corpus2csc(tfidf)\n",
    "U, Sigma, VT = randomized_svd(tfidf_sparse, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.92808679e-01   7.06679855e-01]\n",
      " [ -7.74993893e-24  -2.38175662e-22]\n",
      " [  4.04574539e-01  -3.15748047e-01]\n",
      " [  3.13511892e-01   2.36236614e-01]\n",
      " [  4.40860983e-01  -4.12293569e-01]\n",
      " [  4.38462313e-01  -4.18465087e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.50330866  1.28357349]\n"
     ]
    }
   ],
   "source": [
    "print(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48244458  0.50939981  0.5179025   0.48942421]\n",
      " [ 0.52149895  0.4854238  -0.47612706 -0.5154664 ]]\n"
     ]
    }
   ],
   "source": [
    "print(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [TruncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) below to 1) perform dimensionality reduction on the doc-term matrix, 2) get extra characteristics like explained variance\n",
    "\n",
    "http://scikit-learn.org/stable/modules/decomposition.html#lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.89117442  0.90707552]\n",
      " [ 0.          0.        ]\n",
      " [ 0.60820041 -0.40528582]\n",
      " [ 0.47130514  0.30322705]\n",
      " [ 0.66275013 -0.52920909]\n",
      " [ 0.65914419 -0.53713069]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2, n_iter=1)\n",
    "svd.fit(tfidf_sparse)\n",
    "print(svd.transform(tfidf_sparse)) # dimensionality reduction on original bow matrix: word inference (TODO: right?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48244458  0.50939981  0.5179025   0.48942421]\n",
      " [ 0.52149895  0.4854238  -0.47612706 -0.5154664 ]]\n"
     ]
    }
   ],
   "source": [
    "'''SVD suffers from a problem called \"sign indeterminancy\", which means the\n",
    "sign of the ``components_`` and the output from transform depend on the\n",
    "algorithm and random state. To work around this, fit instances of this\n",
    "class to data once, then keep the instance around to do transformations.'''\n",
    "print(svd.components_) # V^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.50330866  1.28357349]\n"
     ]
    }
   ],
   "source": [
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.075516    0.27269655]\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20772864  0.75013086]\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim [tutorial](https://radimrehurek.com/gensim/tut2.html) mentions that it is \"preferable\" to perform LSA on the tf-idf matrix rather than the bow/term frequency matrix. Why?\n",
    "\n",
    "[sklearn says similarly about tf-idf](http://scikit-learn.org/stable/modules/decomposition.html#lsa): \"While the TruncatedSVD transformer works with any (sparse) feature matrix, using it on tf–idf matrices is recommended over raw frequency counts in an LSA/document processing setting.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On bow matrix with removed stop words, with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\narho_000\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'the')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopword_texts = [(i, word) for i, word in enumerate(dictionary.token2id) if word in stopwords.words('english')]\n",
    "print(stopword_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter stopwords from gensim corpora dictionary\n",
    "dictionary.filter_tokens(bad_ids=[i for i, word in stopword_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clutch': 3, 'gear': 4, 'linux': 2, 'modem': 0, 'steering': 1}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 4), (1, 2), (2, 3)],\n",
       " [(0, 3), (2, 4), (3, 1), (4, 1)],\n",
       " [(1, 3), (2, 1), (3, 4), (4, 3)],\n",
       " [(1, 3), (3, 3), (4, 4)]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = [dictionary.doc2bow(text) for text in texts]\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '-0.518*\"clutch\" + -0.514*\"gear\" + -0.489*\"steering\" + -0.372*\"linux\" + -0.300*\"modem\"'),\n",
       " (1,\n",
       "  '0.648*\"modem\" + 0.587*\"linux\" + -0.331*\"gear\" + -0.317*\"clutch\" + -0.160*\"steering\"')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi = models.LsiModel(bow, id2word=dictionary, num_topics=2) # initialize an LSI transformation\n",
    "lsi.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** Notice the effect of cleaning up _stop words_ when running LSA on the term frequency matrix. Results look more believable! Further, the separation between words _clutch_, _gear_, _steering_ ('cars') and _modem_, _linux_ ('IT') is more clear and the loading magnitudes seem to reflect topic relevance better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.30024393,  0.64782113],\n",
       "       [-0.48911522, -0.15959877],\n",
       "       [-0.37171036,  0.58689171],\n",
       "       [-0.5183769 , -0.31707865],\n",
       "       [-0.513553  , -0.33147438]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.projection.u # left singular vectors (U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.83493413,  6.462851  ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.projection.s # singular values (Ʃ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.37287627, -0.38704343, -0.61723412, -0.5746153 ],\n",
       "       [ 0.62399118,  0.56360224, -0.33338883, -0.42642631]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_lsi = lsi[bow]\n",
    "\n",
    "# https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ#q3-how-do-you-calculate-the-matrix-v-in-lsi-space\n",
    "(corpus2dense(corpus_lsi, len(lsi.projection.s)).T / lsi.projection.s).T # right singular vectors (V^T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, -3.2943372469083361), (1, 4.0327621295130145)]\n",
      "[(0, -3.4195031421177662), (1, 3.6424772099715881)]\n",
      "[(0, -5.4532226067907299), (1, -2.1546423228579714)]\n",
      "[(0, -5.0766883466956418), (1, -2.7559297667574412)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_lsi:\n",
    "     print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On bow matrix with removed stop words, with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "bow_sparse = corpus2csc(bow)\n",
    "U, Sigma, VT = randomized_svd(bow_sparse, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.30024393  0.64782113]\n",
      " [ 0.48911522 -0.15959877]\n",
      " [ 0.37171036  0.58689171]\n",
      " [ 0.5183769  -0.31707865]\n",
      " [ 0.513553   -0.33147438]]\n"
     ]
    }
   ],
   "source": [
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.83493413  6.462851  ]\n"
     ]
    }
   ],
   "source": [
    "print(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.37287627  0.38704342  0.6172341   0.5746153 ]\n",
      " [ 0.6239912   0.56360223 -0.33338883 -0.42642632]]\n"
     ]
    }
   ],
   "source": [
    "print(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [TruncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) below to 1) perform dimensionality reduction on the doc-term matrix, 2) get extra characteristics like explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.65263533  4.18677147]\n",
      " [ 4.32130074 -1.03146305]\n",
      " [ 3.28403659  3.79299367]\n",
      " [ 4.57982573 -2.04923205]\n",
      " [ 4.53720693 -2.14226954]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2, n_iter=1)\n",
    "svd.fit(bow_sparse)\n",
    "print(svd.transform(bow_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.37287627  0.38704342  0.6172341   0.5746153 ]\n",
      " [ 0.6239912   0.56360223 -0.33338883 -0.42642632]]\n"
     ]
    }
   ],
   "source": [
    "'''SVD suffers from a problem called \"sign indeterminancy\", which means the\n",
    "sign of the ``components_`` and the output from transform depend on the\n",
    "algorithm and random state. To work around this, fit instances of this\n",
    "class to data once, then keep the instance around to do transformations.'''\n",
    "print(svd.components_) # V^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.83493413  6.462851  ]\n"
     ]
    }
   ],
   "source": [
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.59557896  8.04969065]\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06152675  0.83157961]\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On tf-idf matrix with removed stop words, with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.936603022962913), (1, 0.19436268823376643), (2, 0.29154403235064963)],\n",
       " [(0, 0.8624176140851579),\n",
       "  (2, 0.47724753317857443),\n",
       "  (3, 0.11931188329464361),\n",
       "  (4, 0.11931188329464361)],\n",
       " [(1, 0.50709255283711),\n",
       "  (2, 0.1690308509457033),\n",
       "  (3, 0.6761234037828132),\n",
       "  (4, 0.50709255283711)],\n",
       " [(1, 0.5144957554275266), (3, 0.5144957554275266), (4, 0.6859943405700354)]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.TfidfModel(bow)\n",
    "tfidf = [model[doc] for doc in bow]\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.593*\"modem\" + 0.441*\"clutch\" + 0.438*\"gear\" + 0.405*\"steering\" + 0.314*\"linux\"'),\n",
       " (1,\n",
       "  '-0.707*\"modem\" + 0.418*\"gear\" + 0.412*\"clutch\" + 0.316*\"steering\" + -0.236*\"linux\"')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi = models.LsiModel(tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation\n",
    "lsi.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same topics and loadings as before stop word removal - which makes sense since tf-idf was robust enough back then to zero-out the loading for 'the'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.59280868 -0.70667985]\n",
      " [ 0.40457454  0.31574805]\n",
      " [ 0.31351189 -0.23623661]\n",
      " [ 0.44086098  0.41229357]\n",
      " [ 0.43846231  0.41846509]]\n"
     ]
    }
   ],
   "source": [
    "print(lsi.projection.u) # left singular vectors (U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.50330866  1.28357349]\n"
     ]
    }
   ],
   "source": [
    "print(lsi.projection.s) # singular values (Ʃ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48244458  0.50939982  0.5179025   0.48942423]\n",
      " [-0.52149894 -0.48542378  0.47612706  0.51546638]]\n"
     ]
    }
   ],
   "source": [
    "corpus_lsi = lsi[tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "\n",
    "# https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ#q3-how-do-you-calculate-the-matrix-v-in-lsi-space\n",
    "print((corpus2dense(corpus_lsi, len(lsi.projection.s)).T / lsi.projection.s).T) # right singular vectors (V^T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7252631168244752), (1, -0.669382223772087)]\n",
      "[(0, 0.76578514191849389), (1, -0.62307711556749124)]\n",
      "[(0, 0.77856731972283222), (1, 0.61114406793638687)]\n",
      "[(0, 0.73575565287652078), (1, 0.66163900269049902)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_lsi: # doc inference: both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "     print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On tf-idf matrix with removed stop words, with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "# requires sparse matrix form of tfidf\n",
    "tfidf_sparse = corpus2csc(tfidf)\n",
    "U, Sigma, VT = randomized_svd(tfidf_sparse, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.59280868  0.70667985]\n",
      " [ 0.40457454 -0.31574805]\n",
      " [ 0.31351189  0.23623661]\n",
      " [ 0.44086098 -0.41229357]\n",
      " [ 0.43846231 -0.41846509]]\n"
     ]
    }
   ],
   "source": [
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.50330866  1.28357349]\n"
     ]
    }
   ],
   "source": [
    "print(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48244458  0.50939981  0.5179025   0.48942421]\n",
      " [ 0.52149895  0.4854238  -0.47612706 -0.5154664 ]]\n"
     ]
    }
   ],
   "source": [
    "print(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [TruncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) below to 1) perform dimensionality reduction on the doc-term matrix, 2) get extra characteristics like explained variance\n",
    "\n",
    "http://scikit-learn.org/stable/modules/decomposition.html#lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.89117442  0.90707552]\n",
      " [ 0.60820041 -0.40528582]\n",
      " [ 0.47130514  0.30322705]\n",
      " [ 0.66275013 -0.52920909]\n",
      " [ 0.65914419 -0.53713069]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2, n_iter=1)\n",
    "svd.fit(tfidf_sparse)\n",
    "print(svd.transform(tfidf_sparse)) # dimensionality reduction on original bow matrix: word inference (? TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48244458  0.50939981  0.5179025   0.48942421]\n",
      " [ 0.52149895  0.4854238  -0.47612706 -0.5154664 ]]\n"
     ]
    }
   ],
   "source": [
    "'''SVD suffers from a problem called \"sign indeterminancy\", which means the\n",
    "sign of the ``components_`` and the output from transform depend on the\n",
    "algorithm and random state. To work around this, fit instances of this\n",
    "class to data once, then keep the instance around to do transformations.'''\n",
    "print(svd.components_) # V^T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Small note:** Up to this point, these results have been exactly the same as _On tf-idf matrix, with scikit-learn_. Explained variance characteristics, however, have changed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.50330866  1.28357349]\n"
     ]
    }
   ],
   "source": [
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01834557  0.32678059]\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05047117  0.89901829]\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder if tf-idf struggles with such a small corpus... I prefer the results here from bow LSA, despite the suggestions from the experts. However, I should probably run a larger analyses before coming to a conclusion on which is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF\n",
    "\n",
    "Non-negative Matrix Factorization\n",
    "\n",
    "\"NMF is an alternative approach to decomposition that **assumes that the data and the components are non-negative**. NMF can be **plugged in instead of PCA or its variants**, in the cases where the data matrix does not contain negative values.\"\n",
    "\n",
    "Base code: http://scikit-learn.org/0.18/auto_examples/applications/topics_extraction_with_nmf_lda.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_nmf_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" + \".join([f'{topic[i]:.3f}*{feature_names[i]}'\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "0.663*gear + 0.661*clutch + 0.558*steering + 0.070*linux + 0.000*modem\n",
      "Topic #1:\n",
      "1.057*modem + 0.424*linux + 0.044*steering + 0.000*gear + 0.000*clutch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "n_topics = 2\n",
    "n_top_words = 20\n",
    "\n",
    "# Fit the NMF model\n",
    "nmf = NMF(n_components=n_topics, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf_sparse.T) # Why transpose? Because NMF automatically clusters the columns of input data - so we transpose to put the words as columns\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = list(dictionary.token2id)\n",
    "print_nmf_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic separation (for this toy example at least) is much cleaner with NMF than LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.55830476  0.06987073  0.66135614  0.66317597]\n",
      " [ 1.05721258  0.04355812  0.42441744  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(nmf.components_) # document scores per topic (H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02899529  0.79336551]\n",
      " [ 0.07958809  0.78535598]\n",
      " [ 0.82681189  0.        ]\n",
      " [ 0.83008648  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(nmf.transform(tfidf_sparse.T)) # word scores per topic (W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pLSA\n",
    "\n",
    "Probabilistic Latent Semantic Analysis\n",
    "\n",
    "\"Previous studies (Ding, Li, and Peng 2006; Gaussier and Goutte 2005) have suggested that **PLSA and Kullback-Leibler (KL) divergence based Non-negative Matrix Factorization (NMF) indeed optimize the same objective function** although they converge to different local minima. It is noteworthy that, as a widely used dimension reduction technique, traditional NMF with Frobenius norm performs well for document clustering and topic modeling (Arora et al. 2012; 2013; Kuang, Choo, and Park 2015), although it\n",
    "lacks explicit probabilistic meaning of factors.\" - https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14469/14409\n",
    "\n",
    "Going to use NMF with Kullback-Leibler divergence as the _distance function_ to approximate pLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in pLSA model:\n",
      "Topic #0:\n",
      "0.667*gear + 0.666*clutch + 0.619*steering + 0.099*linux + 0.000*modem\n",
      "Topic #1:\n",
      "1.064*modem + 0.450*linux + 0.000*gear + 0.000*clutch + 0.000*steering\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "plsa = NMF(n_components=n_topics, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5, beta_loss='kullback-leibler', solver='mu').fit(tfidf_sparse.T)\n",
    "\n",
    "print(\"\\nTopics in pLSA model:\")\n",
    "tfidf_feature_names = list(dictionary.token2id)\n",
    "print_nmf_words(plsa, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Can this be run on BOW?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LDA\n",
    "\n",
    "\"LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA’s topics **can be interpreted as probability distributions over words**... Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA).\" - https://radimrehurek.com/gensim/tut2.html\n",
    "\n",
    "\"Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents.\n",
    "The graphical model of LDA is a three-level Bayesian model\" - http://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.276*\"clutch\" + 0.275*\"gear\" + 0.247*\"steering\" + 0.111*\"linux\" + 0.091*\"modem\"'),\n",
       " (1,\n",
       "  '0.410*\"modem\" + 0.231*\"linux\" + 0.129*\"steering\" + 0.116*\"gear\" + 0.115*\"clutch\"')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lda = models.LdaModel(tfidf, id2word=dictionary, num_topics=2)\n",
    "lda = models.LdaModel(tfidf, id2word=dictionary, num_topics=n_topics, passes=5)\n",
    "lda.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/tmylk/b71bf7d3ec2f203bfce2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "2.277*modem + 1.287*linux + 0.655*steering + 0.588*clutch + 0.587*gear\n",
      "Topic #1:\n",
      "1.725*gear + 1.722*clutch + 1.561*steering + 0.651*linux + 0.522*modem\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method='batch', learning_offset=0, n_jobs=-1)\n",
    "lda.fit(tfidf_sparse.T)\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = list(dictionary.token2id)\n",
    "print_nmf_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.27748806,  0.65542975,  1.28714876,  0.58779908,  0.58711402],\n",
       "       [ 0.52153258,  1.56052125,  0.65067366,  1.72213196,  1.72528476]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_ # topic-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.75569981,  0.24430019],\n",
       "       [ 0.74977371,  0.25022629],\n",
       "       [ 0.20759303,  0.79240697],\n",
       "       [ 0.19702837,  0.80297163]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(tfidf_sparse.T) # document-topic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.811135640598456"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.perplexity(tfidf_sparse.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.652586556243815"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.score(tfidf_sparse.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
